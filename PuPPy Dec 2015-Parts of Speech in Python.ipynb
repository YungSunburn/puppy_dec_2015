{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "First, let's analyze some text...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# “Each of us is full of shit in our own special way. We are all shitty little snowflakes dancing in the universe.” \n",
    "― Lewis Black, Me of Little Faith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/don.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alice's Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/plus.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Taggers/Parsers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tagging and Parsing into Trees is different:\n",
    "\n",
    "* Tagging: Tagging every word [fast]\n",
    "* Parsing: Tagging and puts into Tree [slow]\n",
    "* Chunking: Gives pieces of Trees [medium]\n",
    "* POSH Rules:  *Special* fact and deap and context aware [amazing]\n",
    "\n",
    "Other important *words*:\n",
    "\n",
    "* Probabilistic Parsing\n",
    "* Chart Parsing\n",
    "    * Grammer\n",
    "    * Strategy\n",
    "\n",
    "*NLTK is the mother of all mother of NLP*\n",
    "\n",
    "so many parsers:\n",
    "\n",
    "* pyStatParser (python yay!, little slow, but fun)\n",
    "* Stanford (popular) and btw, online! => http://nlp.stanford.edu:8080/parser/\n",
    "* TextBlob (python yay! NLTK simplification)\n",
    "* clips Pattern (python yay!)\n",
    "* MaltParser (java 1.8)\n",
    "* spaCy (pyython yay!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Parsers/Taggers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \"Each of us is full of stuff in our own special way\"\n",
    "\n",
    "# setup display for demo\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['DISPLAY'] = 'localhost:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyStatParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stat_parser import Parser\n",
    "parser = Parser()\n",
    "parser.parse(sent)\n",
    "tree = parser.parse(sent) # returns nltk Tree instance\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stanford POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Each', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('us', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('full', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('stuff', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('own', 'JJ'),\n",
       " ('special', 'JJ'),\n",
       " ('way', 'NN')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CLASSPATH'] = \"stanford-pos\"\n",
    "os.environ['STANFORD_MODELS'] = \"stanford-pos\"\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "st = StanfordPOSTagger('english-bidirectional-distsim.tagger') \n",
    "st.tag(sent.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "command =  ['/usr/bin/java', '-mx1000m', '-cp', './stanford-postagger.jar', 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', 'english.all.3class.distsim.crf.ser.gz', '-textFile', '/var/folders/zt/0kh07_ls7m53vnmr_kxkn2wdz82_00/T/tmpao2buglj', '-tokenize', 'false', '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/bin/java -mx1000m -cp ./stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model english.all.3class.distsim.crf.ser.gz -textFile /var/folders/zt/0kh07_ls7m53vnmr_kxkn2wdz82_00/T/tmpao2buglj -tokenize false -outputFormatOptions keepEmptySentences -encoding utf8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Each', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('us', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('full', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('stuff', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('own', 'JJ'),\n",
       " ('special', 'JJ'),\n",
       " ('way', 'NN')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(sent)\n",
    "blob.parse()\n",
    "blob.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaltParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('stuff', ['Each', 'of', 'us', 'is', 'full', 'of', Tree('in', [Tree('way', ['our', 'own', 'special'])])])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "mp = nltk.parse.malt.MaltParser(os.getcwd(),\n",
    "                                model_filename=\"engmalt.linear-1.7.mco\")\n",
    "res = mp.parse_one(sent.split())\n",
    "res.tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('stuff', 'NN'), 'nn', ('Each', 'NN')),\n",
       " (('stuff', 'NN'), 'nn', ('of', 'NN')),\n",
       " (('stuff', 'NN'), 'nn', ('us', 'NNS')),\n",
       " (('stuff', 'NN'), 'nn', ('is', 'NNS')),\n",
       " (('stuff', 'NN'), 'nn', ('full', 'NN')),\n",
       " (('stuff', 'NN'), 'nn', ('of', 'NN')),\n",
       " (('stuff', 'NN'), 'prep', ('in', 'IN')),\n",
       " (('in', 'IN'), 'pobj', ('way', 'NN')),\n",
       " (('way', 'NN'), 'nn', ('our', 'NN')),\n",
       " (('way', 'NN'), 'nn', ('own', 'NN')),\n",
       " (('way', 'NN'), 'nn', ('special', 'NN'))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(res.triples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import parse, pprint\n",
    "\n",
    "s = parse(sent,\n",
    "     tokenize = True,  # Tokenize the input, i.e. split punctuation from words.\n",
    "         tags = True,  # Find part-of-speech tags.\n",
    "       chunks = True,  # Find chunk tags, e.g. \"the black cat\" = NP = noun phrase.\n",
    "    relations = True,  # Find relations between chunks.\n",
    "      lemmata = True,  # Find word lemmata.\n",
    "        light = False) \n",
    "pprint(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()\n",
    "parsedData = parser(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 1377 oh\n",
      "lowercased: 1377 oh\n",
      "lemma: 0 \n",
      "shape: 653 xx\n",
      "prefix: 1306 o\n",
      "suffix: 1377 oh\n",
      "log probability: 0.0\n",
      "Brown cluster id: 0\n",
      "----------------------------------------\n",
      "original: 1378 god\n",
      "lowercased: 1378 god\n",
      "lemma: 0 \n",
      "shape: 839 xxx\n",
      "prefix: 1291 g\n",
      "suffix: 1378 god\n",
      "log probability: 0.0\n",
      "Brown cluster id: 0\n",
      "----------------------------------------\n",
      "original: 559 I\n",
      "lowercased: 957 i\n",
      "lemma: 0 \n",
      "shape: 99 X\n",
      "prefix: 559 I\n",
      "suffix: 559 I\n",
      "log probability: 0.0\n",
      "Brown cluster id: 0\n",
      "----------------------------------------\n",
      "original: 1379 sound\n",
      "lowercased: 1379 sound\n",
      "lemma: 0 \n",
      "shape: 1280 xxxx\n",
      "prefix: 945 s\n",
      "suffix: 1380 und\n",
      "log probability: 0.0\n",
      "Brown cluster id: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from spacy.en import English\n",
    "parser = English()\n",
    "parsedData = parser(sent)\n",
    "for i, token in enumerate(parsedData):\n",
    "    print(\"original:\", token.orth, token.orth_)\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    print(\"----------------------------------------\")\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They 0  0  0 \n",
      "told 0  0  0 \n",
      "us 0  0  0 \n",
      "to 0  0  0 \n",
      "duck 0  0  0 \n",
      ". 0  0  0 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = parser(u'They told us to duck.')\n",
    "for word in doc:\n",
    "    print(word.text, word.lemma, word.lemma_, word.tag, word.tag_, word.pos, word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://demos.explosion.ai/displacy/?full=Each of us is full of stuff in our own special way. We are all shitty little snowflakes dancing in the universe.\" target=\"_new\">Interactive Example</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word Langauge Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from visualize_word_graph import draw_graph  \n",
    "draw_graph(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_graph(\"noise\", hypernym=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Alice's Yelp Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_sounds =['The sound in the place is terrible.',\n",
    "            'dining with clatter and the occasional smell of BMW exausts',\n",
    "            'Also, the acoustics are not conducive to having any sort of conversation.']\n",
    "not_bad_sounds = [\"not to sound like a snob\",\n",
    "                  \"at your table and you can tune the sound to whichever game you're interested in\",\n",
    "                  \"oh god I sound old!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. parts of speach for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sound in the place is terrible.\n",
      "          WORD   TAG    CHUNK   ROLE   ID     PNP    LEMMA      \n",
      "                                                                \n",
      "           The   DT     NP      -      -      -      the        \n",
      "         sound   NN     NP ^    -      -      -      sound      \n",
      "            in   IN     PP      -      -      PNP    in         \n",
      "           the   DT     NP      SBJ    1      PNP    the        \n",
      "         place   NN     NP ^    SBJ    1      PNP    place      \n",
      "            is   VBZ    VP      -      1      -      be         \n",
      "      terrible   JJ     ADJP    -      -      -      terrible   \n",
      "             .   .      -       -      -      -      .          \n",
      "dining with clatter and the occasional smell of BMW exausts\n",
      "          WORD   TAG       CHUNK   ROLE   ID     PNP    LEMMA        \n",
      "                                                                     \n",
      "        dining   VBG       VP      -      -      -      dine         \n",
      "          with   IN        PP      -      -      PNP    with         \n",
      "       clatter   NN        NP      -      -      PNP    clatter      \n",
      "           and   CC        NP ^    -      -      PNP    and          \n",
      "           the   DT        NP ^    -      -      PNP    the          \n",
      "    occasional   JJ        NP ^    -      -      PNP    occasional   \n",
      "         smell   NN        NP ^    -      -      PNP    smell        \n",
      "            of   IN        PP      -      -      PNP    of           \n",
      "           BMW   NNP-ORG   NP      SBJ    1      PNP    bmw          \n",
      "       exausts   VBZ       VP      -      1      -      exaust       \n",
      "Also, the acoustics are not conducive to having any sort of conversation.\n",
      "          WORD   TAG    CHUNK    ROLE   ID     PNP    LEMMA          \n",
      "                                                                     \n",
      "          Also   RB     ADVP     -      -      -      also           \n",
      "             ,   ,      -        -      -      -      ,              \n",
      "           the   DT     NP       SBJ    1      -      the            \n",
      "     acoustics   NNS    NP ^     SBJ    1      -      acoustic       \n",
      "           are   VBP    VP       -      1      -      be             \n",
      "           not   RB     ADJP     -      -      -      not            \n",
      "     conducive   JJ     ADJP ^   -      -      -      conducive      \n",
      "            to   TO     VP       -      2      -      to             \n",
      "        having   VBG    VP ^     -      2      -      have           \n",
      "           any   DT     NP       OBJ    2      -      any            \n",
      "          sort   NN     NP ^     OBJ    2      -      sort           \n",
      "            of   IN     PP       -      -      PNP    of             \n",
      "  conversation   NN     NP       -      -      PNP    conversation   \n",
      "             .   .      -        -      -      -      .              \n",
      "not to sound like a snob\n",
      "          WORD   TAG    CHUNK   ROLE   ID     PNP    LEMMA   \n",
      "                                                             \n",
      "           not   RB     VP      -      -      -      not     \n",
      "            to   TO     VP ^    -      -      -      to      \n",
      "         sound   VB     VP ^    -      -      -      sound   \n",
      "          like   IN     PP      -      -      PNP    like    \n",
      "             a   DT     NP      -      -      PNP    a       \n",
      "          snob   NN     NP ^    -      -      PNP    snob    \n",
      "at your table and you can tune the sound to whichever game you're interested in\n",
      "          WORD   TAG    CHUNK   ROLE   ID     PNP    LEMMA        \n",
      "                                                                  \n",
      "            at   IN     PP      -      -      PNP    at           \n",
      "          your   PRP$   NP      SBJ    1      PNP    your         \n",
      "         table   NN     NP ^    SBJ    1      PNP    table        \n",
      "           and   CC     NP ^    SBJ    1      PNP    and          \n",
      "           you   PRP    NP ^    SBJ    1      PNP    you          \n",
      "           can   MD     VP      -      1      -      can          \n",
      "          tune   VB     VP ^    -      1      -      tune         \n",
      "           the   DT     -       -      -      -      the          \n",
      "         sound   JJ     ADJP    -      -      -      sound        \n",
      "            to   TO     -       -      -      -      to           \n",
      "     whichever   WDT    NP      SBJ    2      -      whichever    \n",
      "          game   NN     NP ^    SBJ    2      -      game         \n",
      "           you   PRP    NP ^    SBJ    2      -      you          \n",
      "           're   VBP    VP      -      2      -      be           \n",
      "    interested   JJ     ADJP    -      -      -      interested   \n",
      "            in   IN     PP      -      -      -      in           \n",
      "oh god I sound old!\n",
      "          WORD   TAG    CHUNK   ROLE   ID     PNP    LEMMA   \n",
      "                                                             \n",
      "            oh   UH     -       -      -      -      oh      \n",
      "           god   NN     NP      SBJ    1      -      god     \n",
      "             I   PRP    NP ^    SBJ    1      -      i       \n",
      "         sound   VBP    VP      -      1      -      sound   \n",
      "           old   JJ     ADJP    -      -      -      old     \n",
      "             !   .      -       -      -      -      !       \n"
     ]
    }
   ],
   "source": [
    "from pattern.en import parse, pprint\n",
    "\n",
    "def print_parts(sents):\n",
    "    for sent in sents:\n",
    "        s = parse(sent,\n",
    "             tokenize = True,  # Tokenize the input, i.e. split punctuation from words.\n",
    "                 tags = True,  # Find part-of-speech tags.\n",
    "               chunks = True,  # Find chunk tags, e.g. \"the black cat\" = NP = noun phrase.\n",
    "            relations = True,  # Find relations between chunks.\n",
    "              lemmata = True,  # Find word lemmata.\n",
    "                light = False) \n",
    "        print( sent )\n",
    "        pprint(s)\n",
    "sents = bad_sounds + not_bad_sounds\n",
    "print_parts(bad_sounds + not_bad_sounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penn Treebank Project Chunks <a href=\"tagguide.pdf\">guide</a>\n",
    "\n",
    "#### parts\n",
    "\n",
    "<table class=\"border\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><span class=\"smallcaps\">Tag </span></td>\n",
    "<td><span class=\"smallcaps\">Description </span></td>\n",
    "<td class=\"smallcaps\">Example</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">CC </span></td>\n",
    "<td>conjunction, coordinating</td>\n",
    "<td><em>and, or, but</em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">CD </span></td>\n",
    "<td>cardinal number</td>\n",
    "<td><em>five, three, 13%</em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">DT </span></td>\n",
    "<td>determiner</td>\n",
    "<td><em>the, a, these <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">EX </span></td>\n",
    "<td>existential there</td>\n",
    "<td><em><span style=\"text-decoration: underline;\">there</span> were six boys <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">FW </span></td>\n",
    "<td>foreign word</td>\n",
    "<td><em>mais <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">IN </span></td>\n",
    "<td>conjunction, subordinating or preposition</td>\n",
    "<td><em>of, on, before, unless <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">JJ </span></td>\n",
    "<td>adjective</td>\n",
    "<td><em>nice, easy </em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">JJR </span></td>\n",
    "<td>adjective, comparative</td>\n",
    "<td><em>nicer, easier</em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">JJS </span></td>\n",
    "<td>adjective, superlative</td>\n",
    "<td><em>nicest, easiest <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">LS </span></td>\n",
    "<td>list item marker</td>\n",
    "<td><em>&nbsp;</em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">MD </span></td>\n",
    "<td>verb, modal auxillary</td>\n",
    "<td><em>may, should <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">NN </span></td>\n",
    "<td>noun, singular or mass</td>\n",
    "<td><em>tiger, chair, laughter <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">NNS </span></td>\n",
    "<td>noun, plural</td>\n",
    "<td><em>tigers, chairs, insects <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">NNP </span></td>\n",
    "<td>noun, proper singular</td>\n",
    "<td><em>Germany, God, Alice <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">NNPS </span></td>\n",
    "<td>noun, proper plural</td>\n",
    "<td><em>we met two <span style=\"text-decoration: underline;\">Christmases</span> ago <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">PDT </span></td>\n",
    "<td>predeterminer</td>\n",
    "<td><em><span style=\"text-decoration: underline;\">both</span> his children <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">POS</span></td>\n",
    "<td>possessive ending</td>\n",
    "<td><em>'s</em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">PRP </span></td>\n",
    "<td>pronoun, personal</td>\n",
    "<td><em>me, you, it <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">PRP&#36; </span></td>\n",
    "<td>pronoun, possessive</td>\n",
    "<td><em>my, your, our <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">RB </span></td>\n",
    "<td>adverb</td>\n",
    "<td><em>extremely, loudly, hard&nbsp; <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">RBR </span></td>\n",
    "<td>adverb, comparative</td>\n",
    "<td><em>better <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">RBS </span></td>\n",
    "<td>adverb, superlative</td>\n",
    "<td><em>best <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">RP </span></td>\n",
    "<td>adverb, particle</td>\n",
    "<td><em>about, off, up <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">SYM </span></td>\n",
    "<td>symbol</td>\n",
    "<td><em>&#37; <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">TO </span></td>\n",
    "<td>infinitival to</td>\n",
    "<td><em>what <span style=\"text-decoration: underline;\">to</span> do? <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">UH </span></td>\n",
    "<td>interjection</td>\n",
    "<td><em>oh, oops, gosh <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">VB </span></td>\n",
    "<td>verb, base form</td>\n",
    "<td><em>think <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">VBZ </span></td>\n",
    "<td>verb, 3rd person singular present</td>\n",
    "<td><em>she <span style=\"text-decoration: underline;\">thinks </span><br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">VBP </span></td>\n",
    "<td>verb, non-3rd person singular present</td>\n",
    "<td><em>I <span style=\"text-decoration: underline;\">think </span><br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">VBD </span></td>\n",
    "<td>verb, past tense</td>\n",
    "<td><em>they <span style=\"text-decoration: underline;\">thought </span><br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">VBN </span></td>\n",
    "<td>verb, past participle</td>\n",
    "<td><em>a <span style=\"text-decoration: underline;\">sunken</span> ship <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">VBG </span></td>\n",
    "<td>verb, gerund or present participle</td>\n",
    "<td><em><span style=\"text-decoration: underline;\">thinking</span> is fun <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">WDT </span></td>\n",
    "<td><em>wh</em>-determiner</td>\n",
    "<td><em>which, whatever, whichever <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">WP </span></td>\n",
    "<td><em>wh</em>-pronoun, personal</td>\n",
    "<td><em>what, who, whom <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">WP$$</span></td>\n",
    "<td><em>wh</em>-pronoun, possessive</td>\n",
    "<td><em>whose, whosever <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">WRB</span></td>\n",
    "<td><em>wh</em>-adverb</td>\n",
    "<td><em>where, when <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">. </span></td>\n",
    "<td>punctuation mark, sentence closer</td>\n",
    "<td><em>.;?* <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">, </span></td>\n",
    "<td>punctuation mark, comma</td>\n",
    "<td><em>, <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">: </span></td>\n",
    "<td>punctuation mark, colon</td>\n",
    "<td><em>: <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">( </span></td>\n",
    "<td>contextual separator, left paren</td>\n",
    "<td><em>( <br></em></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">) </span></td>\n",
    "<td>contextual separator, right paren</td>\n",
    "<td><em>) <br></em></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "#### chunks\n",
    "\n",
    "<table class=\"border\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><span class=\"smallcaps\">Tag </span></td>\n",
    "<td><span class=\"smallcaps\">Description </span></td>\n",
    "<td><span class=\"smallcaps\">Words </span></td>\n",
    "<td><span class=\"smallcaps\">Example </span></td>\n",
    "<td align=\"right\">%</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">NP </span></td>\n",
    "<td>noun phrase<span class=\"postag\">&nbsp;</span></td>\n",
    "<td><span class=\"postag\">DT</span>+<span class=\"postag\">RB</span>+<span class=\"postag\">JJ</span>+<span class=\"postag\">NN</span> + <span class=\"postag\">PR</span></td>\n",
    "<td><em>the strange bird</em></td>\n",
    "<td align=\"right\">&nbsp;51</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">PP </span></td>\n",
    "<td>prepositional phrase</td>\n",
    "<td><span class=\"postag\">TO</span>+<span class=\"postag\">IN </span></td>\n",
    "<td><em>in between</em></td>\n",
    "<td align=\"right\">&nbsp;19</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">VP&nbsp; </span></td>\n",
    "<td>verb phrase&nbsp;</td>\n",
    "<td><span class=\"postag\">RB</span>+<span class=\"postag\">MD</span>+<span class=\"postag\">VB&nbsp; </span></td>\n",
    "<td><em>was looking<br></em></td>\n",
    "<td align=\"right\">9</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">ADVP</span></td>\n",
    "<td>adverb phrase</td>\n",
    "<td><span class=\"postag\">RB</span></td>\n",
    "<td><em>also<br></em></td>\n",
    "<td align=\"right\">&nbsp;6</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">ADJP</span></td>\n",
    "<td>adjective phrase<span class=\"postag\">&nbsp;</span></td>\n",
    "<td><span class=\"postag\">CC</span>+<span class=\"postag\">RB</span>+<span class=\"postag\">JJ</span></td>\n",
    "<td><em>warm and cosy</em></td>\n",
    "<td align=\"right\">&nbsp;3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">SBAR</span></td>\n",
    "<td>subordinating conjunction&nbsp;</td>\n",
    "<td><span class=\"postag\">IN</span></td>\n",
    "<td><em><span style=\"text-decoration: underline;\">whether</span> or not<br></em></td>\n",
    "<td align=\"right\">3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">PRT </span></td>\n",
    "<td>particle</td>\n",
    "<td><span class=\"postag\">RP</span></td>\n",
    "<td><em><span style=\"text-decoration: underline;\">up</span> the stairs</em></td>\n",
    "<td align=\"right\">&nbsp;1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span class=\"postag\">INTJ</span></td>\n",
    "<td>interjection</td>\n",
    "<td><span class=\"postag\">UH</span></td>\n",
    "<td><em>hello</em><em><br></em></td>\n",
    "<td align=\"right\">&nbsp;0</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. seach for patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sound in the place is terrible.\n",
      "Tagged Sent: [Sentence(\"The/DT/B-NP/O sound/NN/I-NP/O in/IN/B-PP/B-PNP the/DT/B-NP/I-PNP place/NN/I-NP/I-PNP is/VBZ/B-VP/O terrible/JJ/B-ADJP/O ././O/O\")]\n",
      "Verbs: [Match(words=[Word('is/VBZ')])]\n",
      "ADJP: [Match(words=[Word('terrible/JJ')])]\n",
      "Nouns: [Match(words=[Word('sound/NN')]), Match(words=[Word('place/NN')])]\n",
      "dining with clatter and the occasional smell of BMW exausts\n",
      "Tagged Sent: [Sentence(\"dining/VBG/B-VP/O with/IN/B-PP/B-PNP clatter/NN/B-NP/I-PNP and/CC/I-NP/I-PNP the/DT/I-NP/I-PNP occasional/JJ/I-NP/I-PNP smell/NN/I-NP/I-PNP of/IN/B-PP/B-PNP BMW/NNP-ORG/B-NP/I-PNP exausts/VBZ/B-VP/O\")]\n",
      "Verbs: [Match(words=[Word('dining/VBG')]), Match(words=[Word('exausts/VBZ')])]\n",
      "ADJP: []\n",
      "Nouns: [Match(words=[Word('clatter/NN')]), Match(words=[Word('smell/NN')])]\n",
      "Also, the acoustics are not conducive to having any sort of conversation.\n",
      "Tagged Sent: [Sentence(\"Also/RB/B-ADVP/O ,/,/O/O the/DT/B-NP/O acoustics/NNS/I-NP/O are/VBP/B-VP/O not/RB/B-ADJP/O conducive/JJ/I-ADJP/O to/TO/B-VP/O having/VBG/I-VP/O any/DT/B-NP/O sort/NN/I-NP/O of/IN/B-PP/B-PNP conversation/NN/B-NP/I-PNP ././O/O\")]\n",
      "Verbs: [Match(words=[Word('are/VBP')]), Match(words=[Word('having/VBG')])]\n",
      "ADJP: [Match(words=[Word('not/RB'), Word('conducive/JJ')])]\n",
      "Nouns: [Match(words=[Word('sort/NN')]), Match(words=[Word('conversation/NN')])]\n",
      "not to sound like a snob\n",
      "Tagged Sent: [Sentence(\"not/RB/B-VP/O to/TO/I-VP/O sound/VB/I-VP/O like/IN/B-PP/B-PNP a/DT/B-NP/I-PNP snob/NN/I-NP/I-PNP\")]\n",
      "Verbs: [Match(words=[Word('sound/VB')])]\n",
      "ADJP: []\n",
      "Nouns: [Match(words=[Word('snob/NN')])]\n",
      "at your table and you can tune the sound to whichever game you're interested in\n",
      "Tagged Sent: [Sentence(\"at/IN/B-PP/B-PNP your/PRP$/B-NP/I-PNP table/NN/I-NP/I-PNP and/CC/I-NP/I-PNP you/PRP/I-NP/I-PNP can/MD/B-VP/O tune/VB/I-VP/O the/DT/O/O sound/JJ/B-ADJP/O to/TO/O/O whichever/WDT/B-NP/O game/NN/I-NP/O you/PRP/I-NP/O 're/VBP/B-VP/O interested/JJ/B-ADJP/O in/IN/B-PP/O\")]\n",
      "Verbs: [Match(words=[Word('tune/VB')]), Match(words=[Word(\"'re/VBP\")])]\n",
      "ADJP: [Match(words=[Word('sound/JJ')]), Match(words=[Word('interested/JJ')])]\n",
      "Nouns: [Match(words=[Word('table/NN')]), Match(words=[Word('game/NN')])]\n",
      "oh god I sound old!\n",
      "Tagged Sent: [Sentence(\"oh/UH/O/O god/NN/B-NP/O I/PRP/I-NP/O sound/VBP/B-VP/O old/JJ/B-ADJP/O !/./O/O\")]\n",
      "Verbs: [Match(words=[Word('sound/VBP')])]\n",
      "ADJP: [Match(words=[Word('old/JJ')])]\n",
      "Nouns: [Match(words=[Word('god/NN')])]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import parsetree\n",
    "from pattern.search import search\n",
    "\n",
    "for sent in sents:\n",
    "    t = parsetree(sent)\n",
    "\n",
    "    print( sent )\n",
    "    print( \"Tagged Sent:\", t )\n",
    "    print( \"Verbs:\", search('VB*', t) ) # verbs\n",
    "    print( \"ADJP:\", search('ADJP', t) ) # verbs   \n",
    "    print( \"Nouns:\", search('NN', t) ) # all nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. create similar word list (stemming + synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from pattern.en import parsetree\n",
    "from pattern.search import taxonomy, WordNetClassifier, search\n",
    "\n",
    "taxonomy.classifiers.append(WordNetClassifier())\n",
    "\n",
    "def get_parts(word, pos, recursive=False):\n",
    "    parts = [word, ]\n",
    "    parts += taxonomy.children(word, pos=pos, recursive=recursive)\n",
    "    parts += taxonomy.parents(word, pos=pos, recursive=recursive)\n",
    "    return parts\n",
    "\n",
    "def word_search(t, word, pos):\n",
    "    parts = get_parts(word, pos)\n",
    "    results = search(pos, t)\n",
    "    for result in results:\n",
    "        #  print result.string, parts\n",
    "        if any(x in result.string.split() for x in parts):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def run_a_rule(sent, word, pos):\n",
    "    t = parsetree(sent)\n",
    "    return word_search(t, word, pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"1. 'sound' is a NN\"\n",
    "print run_a_rule(sents[0], 'noise', 'NN')\n",
    "\n",
    "print \"2. clatter is a NN\"\n",
    "print run_a_rule(sents[1], 'noise', 'NN')\n",
    "\n",
    "print \"3. acoustics is NNS and RB Not\"\n",
    "print run_a_rule(sents[2], 'acoustics', 'NNS') and run_a_rule(sents[2], 'not', 'RB')\n",
    "\n",
    "print \"4. sound is a VB\"\n",
    "print run_a_rule(sents[3], 'noise', 'VB*') \n",
    "\n",
    "print \"5. Sounds is JJ\"\n",
    "print run_a_rule(sents[4], 'sound', 'JJ') \n",
    "\n",
    "print \"6. sound is VBP\"\n",
    "print run_a_rule(sents[5], 'noise', 'VB*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. create a feature extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ext_func(tgt):\n",
    "    return bool(not (run_a_rule(tgt, 'noise', 'VB*') and not run_a_rule(tgt, 'sound', 'JJ'))\n",
    "                and (run_a_rule(tgt, 'noise', 'NN') or run_a_rule(tgt, 'acoustics', 'NNS') or\n",
    "                        (run_a_rule(tgt, 'acoustics', 'NNS') and run_a_rule(tgt, 'not', 'RB'))))\n",
    "        \n",
    "print \"bad noises in review:\"\n",
    "for sent in bad_sounds:\n",
    "    print \"\\t\" + sent\n",
    "    assert(ext_func(sent) == True)\n",
    "print\n",
    "print \"no mention of bad noises:\"\n",
    "for sent in not_bad_sounds:\n",
    "    print \"\\t\" + sent\n",
    "    assert(ext_func(sent) == False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pickle\n",
    "from lxml import etree\n",
    "from StringIO import StringIO\n",
    "\n",
    "zf = zipfile.ZipFile('nhtsa_as_xml.zip', 'r')\n",
    "nhtsa_injured = zf.read('nhtsa_injured.xml')\n",
    "nhtsa_not_injured = zf.read('nhtsa_not_injured.xml')\n",
    "xml_injured = etree.parse(StringIO(nhtsa_injured))\n",
    "xml_not_injured = etree.parse(StringIO(nhtsa_not_injured))\n",
    "\n",
    "\n",
    "def injured(l):\n",
    "    return ['0' != str(x) and 'injured' or 'notinjured' for x in l]\n",
    "\n",
    "\n",
    "def data(x):\n",
    "    out = [x.xpath(\"//rows/row/@c1\"),\n",
    "           injured(x.xpath(\"//rows/row/@c8\")),\n",
    "           x.xpath(\"//rows/row/@c2\")]\n",
    "    return list(reversed(zip(*out)))\n",
    "\n",
    "\n",
    "xml_injured_data = data(xml_injured)[:800]\n",
    "xml_not_injured_data = data(xml_not_injured)[:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_injured_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_word_graph import draw_graph  \n",
    "draw_graph(\"injury\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from pattern.search import taxonomy, search\n",
    "\n",
    "taxonomy.append('dislocated', type='injury')\n",
    "taxonomy.append('sustained', type='injury')\n",
    "taxonomy.append('burn', type='injury')\n",
    "taxonomy.append('injury', type='hurt')\n",
    "\n",
    "\n",
    "def check_sustained(text):\n",
    "    if len(search('HURT', text)) > 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def feats(text):\n",
    "    words = text.replace(\".\", \"\").split()\n",
    "    out = dict([(word, True) for word in words])\n",
    "    if 'SUSTAINED' in out:\n",
    "        del out['SUSTAINED']\n",
    "    out['rule(SUSTAINED)'] = check_sustained(text)\n",
    "    return out\n",
    "    \n",
    "negcutoff = len(xml_not_injured_data)*3/4\n",
    "poscutoff = len(xml_injured_data)*3/4\n",
    " \n",
    "not_inj_data = xml_not_injured_data[:negcutoff] + xml_injured_data[:poscutoff]\n",
    "inj_data = xml_not_injured_data[negcutoff:] + xml_injured_data[poscutoff:]    \n",
    "    \n",
    "negfeats = [(feats(f[2]), 'not') for f in not_inj_data]\n",
    "posfeats = [(feats(f[2]), 'injure') for f in inj_data]\n",
    "egcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features(n=100)\n",
    "\n",
    "\n",
    "classifier.classify(feats(\"HE SUSTAINED INJURY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSH Syntax Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converts:\n",
    "\n",
    "    return bool(not (run_a_rule(tgt, 'noise', 'VB*') and not run_a_rule(tgt, 'sound', 'JJ'))\n",
    "                and (run_a_rule(tgt, 'noise', 'NN') or run_a_rule(tgt, 'acoustics', 'NNS') or\n",
    "                        (run_a_rule(tgt, 'acoustics', 'NNS') and run_a_rule(tgt, 'not', 'RB'))))\n",
    "\n",
    "To:\n",
    "\n",
    "    SENT: !VB*(noise+3) and !JJ(sound+3) ) and (NN(noise+2) | NNS(acoustics) | (NNS(acoustics) & RB(not)))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Natural Language API\n",
    "\n",
    "Helpful tutorial on Google API(s) https://www.programmableweb.com/news/how-to-start-using-google-cloud-natural-language-api/how-to/2016/09/01#apiu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "import httplib2\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import os\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']  =  os.path.expanduser(\"~/my_key_from_google.json\")\n",
    "\n",
    "DISCOVERY_URL = ('https://{api}.googleapis.com/'\n",
    "                '$discovery/rest?version={apiVersion}')\n",
    "\n",
    "http = httplib2.Http()\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default().create_scoped(\n",
    " ['https://www.googleapis.com/auth/cloud-platform'])\n",
    "\n",
    "http=httplib2.Http()\n",
    "credentials.authorize(http)\n",
    "\n",
    "service = discovery.build('language', 'v1beta1',\n",
    "                       http=http, discoveryServiceUrl=DISCOVERY_URL)\n",
    "\n",
    "service_request = service.documents().analyzeSyntax(\n",
    "body={\n",
    " 'document': {\n",
    "    'type': 'PLAIN_TEXT',\n",
    "    'content': sent\n",
    " }\n",
    "})\n",
    "\n",
    "response = service_request.execute()\n",
    "\n",
    "for token in response['tokens']:\n",
    "    print(\"{} -> {}\".format(token['text']['content'],token['partOfSpeech']['tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['tokens'][0]['partOfSpeech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSH Library \n",
    "\n",
    "Comming soon to: https://github.com/brianray/posh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/me.png\">\n",
    "### About Me\n",
    "\n",
    "\n",
    "* Deloitte Enterprise Science brray (at) deloitte  dot com\n",
    "* ChiPy (Chicago Python User Group) brianhray@gmail.com\n",
    "* LinkedIn: https://www.linkedin.com/in/brianray\n",
    "* Twitter: <a href=\"https://twitter.com/brianray\" class=\"twitter-follow-button\" data-show-count=\"false\" data-size=\"large\">Follow @brianray</a>\n",
    "<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Copy of this presentation found here: https://github.com/brianray/puppy_dec_2015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
